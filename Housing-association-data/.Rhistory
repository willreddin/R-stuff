values <- xpathSApply(IndividualHtml, "//strong/following-sibling::text()[1]", xmlValue)
#write these to a data frame
d <- data.frame(values, row.names=headings)
#write table to file with url as name
write.table(d, file= "data.txt", append=TRUE, sep= ",", append=FALSE, row.names=TRUE)
}
parsePage(links[1])
parsePage <- function(links) {
#set html to the url for individual HA record
IndividualHtml <- htmlTreeParse(links, useInternalNodes=T)
#set headings for individual HA page
headings <- xpathSApply(IndividualHtml, "//strong", xmlValue)
#get the non strong values
values <- xpathSApply(IndividualHtml, "//strong/following-sibling::text()[1]", xmlValue)
#write these to a data frame
d <- data.frame(values, row.names=headings)
#write table to file with url as name
write.table(d, file= "data.txt", sep= ",", append=FALSE, row.names=TRUE)
}
parsePage(links[1])
parsePage <- function(links) {
#set html to the url for individual HA record
IndividualHtml <- htmlTreeParse(links, useInternalNodes=T)
#set headings for individual HA page
headings <- xpathSApply(IndividualHtml, "//strong", xmlValue)
#get the non strong values
values <- xpathSApply(IndividualHtml, "//strong/following-sibling::text()[1]", xmlValue)
#write these to a data frame
d <- data.frame(values, row.names=headings)
#write table to file with url as name
write.table(d, file= "data.txt", append=TRUE, sep= ",", append=FALSE, row.names=TRUE)
}
lapply(links, parsePage)
parsePage <- function(links) {
#set html to the url for individual HA record
IndividualHtml <- htmlTreeParse(links, useInternalNodes=T)
#set headings for individual HA page
headings <- xpathSApply(IndividualHtml, "//strong", xmlValue)
#get the non strong values
values <- xpathSApply(IndividualHtml, "//strong/following-sibling::text()[1]", xmlValue)
#write these to a data frame
d <- data.frame(values, row.names=headings)
#write table to file with url as name
write.table(d, file= "data.txt", append=TRUE, sep= ",", row.names=TRUE)
}
lapply(links, parsePage)
parsePage(links[50])
read.csv("data.txt")
xpathSapply(IndividualHtml, "title", xmlValue)
xpathSApply(IndividualHtml, "title", xmlValue)
library(XML)
#set WD
setwd("~/Documents/R")
#get all the urls for HAs from this page: "http://www.theguardian.com/housing-network/2012/jun/08/housing-association-profiles"
indexUrl <- "http://www.theguardian.com/housing-network/2012/jun/08/housing-association-profiles"
#set html to the url for individual HA record
html <- htmlTreeParse(indexUrl, useInternalNodes=T)
#use xpath to get all of the links for each HA record
links <- xpathSApply(html, "//p//a", xmlGetAttr, "href")
#loop through each link, and return the data table for each HA
for (i in length(links)){
values <- xpathSApply(htmlTreeParse(links[i], useInternalNodes=T), "//strong/following-sibling::text()[1]", xmlValue)
headings <- xpathSApply(htmlTreeParse(links[i], useInternalNodes=T), "//strong", xmlValue)
data.frame(values, row.names= headings
}
IndividualHtml <- htmlTreeParse(links, useInternalNodes=T)
title <- xpathSApply(IndividualHtml, "title", xmlValue)
title
title <- xpathSApply(IndividualHtml, "//title", xmlValue)
title
title <- xpathSApply(IndividualHtml, "/html/body/div[6]/article/header/div[1]/div/div/h1", xmlValue)
title
title <- xpathSApply(IndividualHtml, "//h1", xmlValue)
title
title <- xpathSApply(IndividualHtml, "/h1", xmlValue)
title
title <- xpathSApply(IndividualHtml, "/h1[class="content__headline js-score"]", xmlValue)
title <- xpathSApply(IndividualHtml, "/h1[@class="content__headline js-score"]", xmlValue)
title <- xpathSApply(IndividualHtml, "/html/body/div[6]/article/header/div[1]/div/div/h1", xmlValue)
title
title <- xpathSApply(IndividualHtml, "/html/body/div[6]/article/header/div["content__headline js-score"]/div/div/h1", xmlValue)
title <- xpathSApply(IndividualHtml, "/html/body/div[6]/article/header/div[content__headline js-score]/div/div/h1", xmlValue)
title <- xpathSApply(IndividualHtml, "[contains(concat(" ", normalize-space(@class), " "), " content__headline js-score ")]", xmlValue)
title <- xpathSApply(IndividualHtml, "//h1[1]", xmlValue)
title
title <- xpathSApply(IndividualHtml, "//h1[@class=1]", xmlValue)
title
title <- xpathSApply(IndividualHtml, "//h1[@class="headline"]", xmlValue)
title <- xpathSApply(IndividualHtml, "//h1[@class=]", xmlValue)
title <- xpathSApply(IndividualHtml, "//h1", xmlValue)
TITLE
title
title <- xpathSApply(IndividualHtml, "//header/h1", xmlValue)
title
title <- xpathSApply(IndividualHtml, "//header/div/div/div/div/h1", xmlValue)
title
title <- xpath(IndividualHtml, "//h1", xmlValue)
?xpathSApply
title <- xpathApply(IndividualHtml, "//h1", xmlValue)
title
title <- xpathApply(IndividualHtml, "/html/body/div[6]/article/header/div[1]/div/div/h1", xmlValue)
title
title <- xpathSApply(IndividualHtml, "/html/body/div[6]/article/header/div[1]/div/div/h1", xmlValue)
title
title <- xpathSApply(IndividualHtml, "//div[6]/article/header/div[1]/div/div/h1", xmlValue)
title
title <- xpathSApply(IndividualHtml, "//article/header/div[1]/div/div/h1", xmlValue)
title
title <- xpathSApply(IndividualHtml, "//header/div[1]/div/div/h1", xmlValue)
title
title <- xpathSApply(IndividualHtml, "//div[1]/div/div/h1", xmlValue)
title
title <- xpathSApply(IndividualHtml, "//div/div/h1", xmlValue)
title
view(title)
title[1]
title[2]
title[3]
title[4]
title[1:100]
IndividualHtml
headings
headings <- xpathSApply(IndividualHtml, "//strong", xmlValue)
headings
IndividualHtml <- htmlTreeParse(links[1], useInternalNodes=T)
IndividualHtml
title <- xpathSApply(IndividualHtml, "//h1[@class="content__headline js-score"]", xmlValue)
IndividualHtml[[1][1]]
IndividualHtml[[1]][[1]]
values <- xpathSApply(IndividualHtml, "//strong/following-sibling::text()[1]", xmlValue)
values
xmlParse(IndividualHtml)
xmlParse(IndividualHtml, isHTML=TRUE)
title <- xpathSApply(IndividualHtml, "//h1[1]", xmlValue)
title
title <- sub("\n", "",  xpathSApply(IndividualHtml, "//h1[1]", xmlValue))
title
title <- gsub("\n", "",  xpathSApply(IndividualHtml, "//h1[1]", xmlValue))
title
paste(title, ".csv", sep="")
parsePage <- function(links) {
#set html to the url for individual HA record
IndividualHtml <- htmlTreeParse(links, useInternalNodes=T)
#set headings for individual HA page
headings <- xpathSApply(IndividualHtml, "//strong", xmlValue)
#get the non strong values
values <- xpathSApply(IndividualHtml, "//strong/following-sibling::text()[1]", xmlValue)
#parse the title of the housing association from the page
title <- gsub("\n", "",  xpathSApply(IndividualHtml, "//h1[1]", xmlValue))
#write these to a data frame
d <- data.frame(values, row.names= headings)
#write table to file with url as name
write.csv(d, file= paste(title, ".csv", sep=""), append=FALSE, sep= ",", na="NA", row.names=TRUE)
}
parsePage(links[1])
parsePage <- function(links) {
#set html to the url for individual HA record
IndividualHtml <- htmlTreeParse(links, useInternalNodes=T)
#set headings for individual HA page
headings <- xpathSApply(IndividualHtml, "//strong", xmlValue)
#get the non strong values
values <- xpathSApply(IndividualHtml, "//strong/following-sibling::text()[1]", xmlValue)
#parse the title of the housing association from the page
title <- gsub("\n", "",  xpathSApply(IndividualHtml, "//h1[1]", xmlValue))
#write these to a data frame
d <- data.frame(values, row.names= headings)
#write table to file with url as name
write.csv(d, file= paste(title, ".csv", sep=""), na="NA", row.names=TRUE)
}
library(XML)
#set WD
setwd("~/Documents/R")
#get all the urls for HAs from this page: "http://www.theguardian.com/housing-network/2012/jun/08/housing-association-profiles"
indexUrl <- "http://www.theguardian.com/housing-network/2012/jun/08/housing-association-profiles"
#set html to the url for individual HA record
html <- htmlTreeParse(indexUrl, useInternalNodes=T)
#use xpath to get all of the links for each HA record
links <- xpathSApply(html, "//p//a", xmlGetAttr, "href")
#loop through each link, and return the data table for each HA
parsePage <- function(links) {
#set html to the url for individual HA record
IndividualHtml <- htmlTreeParse(links, useInternalNodes=T)
#set headings for individual HA page
headings <- xpathSApply(IndividualHtml, "//strong", xmlValue)
#get the non strong values
values <- xpathSApply(IndividualHtml, "//strong/following-sibling::text()[1]", xmlValue)
#parse the title of the housing association from the page
title <- gsub("\n", "",  xpathSApply(IndividualHtml, "//h1[1]", xmlValue))
#write these to a data frame
d <- data.frame(values, row.names= headings)
#write table to file with url as name
write.csv(d, file= paste(title, ".csv", sep=""), na="NA", row.names=TRUE)
}
parsePage(links[1])
getwd()
library(XML)
#set WD
setwd("~/Documents/R/Housing-association-data")
#get all the urls for HAs from this page: "http://www.theguardian.com/housing-network/2012/jun/08/housing-association-profiles"
indexUrl <- "http://www.theguardian.com/housing-network/2012/jun/08/housing-association-profiles"
#set html to the url for individual HA record
html <- htmlTreeParse(indexUrl, useInternalNodes=T)
#use xpath to get all of the links for each HA record
links <- xpathSApply(html, "//p//a", xmlGetAttr, "href")
#loop through each link, and return the data table for each HA
library(XML)
#set WD
setwd("~/Documents/R/Housing-association-data")
#get all the urls for HAs from this page: "http://www.theguardian.com/housing-network/2012/jun/08/housing-association-profiles"
indexUrl <- "http://www.theguardian.com/housing-network/2012/jun/08/housing-association-profiles"
#set html to the url for individual HA record
html <- htmlTreeParse(indexUrl, useInternalNodes=T)
#use xpath to get all of the links for each HA record
links <- xpathSApply(html, "//p//a", xmlGetAttr, "href")
#loop through each link, and return the data table for each HA
parsePage <- function(links) {
#set html to the url for individual HA record
IndividualHtml <- htmlTreeParse(links, useInternalNodes=T)
#set headings for individual HA page
headings <- xpathSApply(IndividualHtml, "//strong", xmlValue)
#get the non strong values
values <- xpathSApply(IndividualHtml, "//strong/following-sibling::text()[1]", xmlValue)
#parse the title of the housing association from the page
title <- gsub("\n", "",  xpathSApply(IndividualHtml, "//h1[1]", xmlValue))
#write these to a data frame
d <- data.frame(values, row.names= headings)
#write table to file with url as name
write.csv(d, file= paste(title, ".csv", sep=""), na="NA", row.names=TRUE)
}
lapply(links, parsePage)
library(XML)
#set WD
setwd("~/Documents/R/Housing-association-data")
#get all the urls for HAs from this page: "http://www.theguardian.com/housing-network/2012/jun/08/housing-association-profiles"
indexUrl <- "http://www.theguardian.com/housing-network/2012/jun/08/housing-association-profiles"
#set html to the url for individual HA record
html <- htmlTreeParse(indexUrl, useInternalNodes=T)
#use xpath to get all of the links for each HA record
links <- xpathSApply(html, "//p//a", xmlGetAttr, "href")
#loop through each link, and return the data table for each HA
parsePage <- function(links) {
#set html to the url for individual HA record
IndividualHtml <- htmlTreeParse(links, useInternalNodes=T)
#set headings for individual HA page
headings <- xpathSApply(IndividualHtml, "//strong", xmlValue)
#get the non strong values
values <- xpathSApply(IndividualHtml, "//strong/following-sibling::text()[1]", xmlValue)
#parse the title of the housing association from the page
title <- gsub("\n", "",  xpathSApply(IndividualHtml, "//h1[1]", xmlValue))
#write these to a data frame
d <- data.frame(values, row.names= headings)
#write table to file with url as name
write.csv(d, file= paste(title, ".csv", sep=""), na="NA")
}
lapply(links, parsePage)
library(XML)
#set WD
setwd("~/Documents/R/Housing-association-data")
#get all the urls for HAs from this page: "http://www.theguardian.com/housing-network/2012/jun/08/housing-association-profiles"
indexUrl <- "http://www.theguardian.com/housing-network/2012/jun/08/housing-association-profiles"
#set html to the url for individual HA record
html <- htmlTreeParse(indexUrl, useInternalNodes=T)
#use xpath to get all of the links for each HA record
links <- xpathSApply(html, "//p//a", xmlGetAttr, "href")
#loop through each link, and return the data table for each HA
parsePage <- function(links) {
#set html to the url for individual HA record
IndividualHtml <- htmlTreeParse(links, useInternalNodes=T)
#set headings for individual HA page
headings <- xpathSApply(IndividualHtml, "//strong", xmlValue)
#get the non strong values
values <- xpathSApply(IndividualHtml, "//strong/following-sibling::text()[1]", xmlValue)
#parse the title of the housing association from the page
title <- gsub("\n", "",  xpathSApply(IndividualHtml, "//h1[1]", xmlValue))
#write these to a data frame
d <- data.frame(values, headings)
#write table to file with url as name
write.csv(d, file= paste(title, ".csv", sep=""), na="NA")
}
lapply(links, parsePage)
read.csv("A2Dominion Group.csv")
lapply(links, parsePage)
parsePage(links[2])
links[2]
links[3]
?data.frame
library(XML)
#set WD
setwd("~/Documents/R/Housing-association-data")
#get all the urls for HAs from this page: "http://www.theguardian.com/housing-network/2012/jun/08/housing-association-profiles"
indexUrl <- "http://www.theguardian.com/housing-network/2012/jun/08/housing-association-profiles"
#set html to the url for individual HA record
html <- htmlTreeParse(indexUrl, useInternalNodes=T)
#use xpath to get all of the links for each HA record
links <- xpathSApply(html, "//p//a", xmlGetAttr, "href")
#loop through each link, and return the data table for each HA
parsePage <- function(links) {
#set html to the url for individual HA record
IndividualHtml <- htmlTreeParse(links, useInternalNodes=T)
#set headings for individual HA page
headings <- xpathSApply(IndividualHtml, "//strong", xmlValue)
#get the non strong values
values <- xpathSApply(IndividualHtml, "//strong/following-sibling::text()[1]", xmlValue)
#parse the title of the housing association from the page
title <- gsub("\n", "",  xpathSApply(IndividualHtml, "//h1[1]", xmlValue))
#write these to a data frame
d <- data.frame(values, headings, check.rows=FALSE)
#write table to file with url as name
write.csv(d, file= paste(title, ".csv", sep=""), na="NA")
}
lapply(links, parsePage)
?lapply
sapply(links, parsePage)
links[2]
?data.frame
headings
IndividualHtml <- htmlTreeParse("http://www.theguardian.com/housing-network/2012/jun/08/accent-group", useInternalNodes=T)
values <- xpathSApply(IndividualHtml, "//strong/following-sibling::text()[1]", xmlValue)
values
library(XML)
#set WD
setwd("~/Documents/R/Housing-association-data")
#get all the urls for HAs from this page: "http://www.theguardian.com/housing-network/2012/jun/08/housing-association-profiles"
indexUrl <- "http://www.theguardian.com/housing-network/2012/jun/08/housing-association-profiles"
#set html to the url for individual HA record
html <- htmlTreeParse(indexUrl, useInternalNodes=T)
#use xpath to get all of the links for each HA record
links <- xpathSApply(html, "//p//a", xmlGetAttr, "href")
#loop through each link, and return the data table for each HA
parsePage <- function(links) {
#set html to the url for individual HA record
IndividualHtml <- htmlTreeParse(links, useInternalNodes=T)
#set headings for individual HA page
headings <- xpathSApply(IndividualHtml, "//strong", xmlValue)
#get the non strong values
values <- xpathSApply(IndividualHtml, "//strong/following-sibling::text()[1]", xmlValue)
#parse the title of the housing association from the page
title <- gsub("\n", "",  xpathSApply(IndividualHtml, "//h1[1]", xmlValue))
#write these to a data frame
d <- tryCatch(data.frame(values, headings),  error=function(e) NULL))
#write table to file with url as name
write.csv(d, file= paste(title, ".csv", sep=""), na="NA")
}
library(XML)
#set WD
setwd("~/Documents/R/Housing-association-data")
#get all the urls for HAs from this page: "http://www.theguardian.com/housing-network/2012/jun/08/housing-association-profiles"
indexUrl <- "http://www.theguardian.com/housing-network/2012/jun/08/housing-association-profiles"
#set html to the url for individual HA record
html <- htmlTreeParse(indexUrl, useInternalNodes=T)
#use xpath to get all of the links for each HA record
links <- xpathSApply(html, "//p//a", xmlGetAttr, "href")
#loop through each link, and return the data table for each HA
parsePage <- function(links) {
#set html to the url for individual HA record
IndividualHtml <- htmlTreeParse(links, useInternalNodes=T)
#set headings for individual HA page
headings <- xpathSApply(IndividualHtml, "//strong", xmlValue)
#get the non strong values
values <- xpathSApply(IndividualHtml, "//strong/following-sibling::text()[1]", xmlValue)
#parse the title of the housing association from the page
title <- gsub("\n", "",  xpathSApply(IndividualHtml, "//h1[1]", xmlValue))
#write these to a data frame
d <- tryCatch(data.frame(values, headings),  error=function(e) NULL)
#write table to file with url as name
write.csv(d, file= paste(title, ".csv", sep=""), na="NA")
}
lapply(links, parsePage)
something <- read.csv("Asra Group.csv")
this <- data.frame(something)
view(this)
View(this)
library(XML)
#set WD
setwd("~/Documents/R/Housing-association-data")
#get all the urls for HAs from this page: "http://www.theguardian.com/housing-network/2012/jun/08/housing-association-profiles"
indexUrl <- "http://www.theguardian.com/housing-network/2012/jun/08/housing-association-profiles"
#set html to the url for individual HA record
html <- htmlTreeParse(indexUrl, useInternalNodes=T)
#use xpath to get all of the links for each HA record
links <- xpathSApply(html, "//p//a", xmlGetAttr, "href")
#loop through each link, and return the data table for each HA
parsePage <- function(links) {
#set html to the url for individual HA record
IndividualHtml <- htmlTreeParse(links, useInternalNodes=T)
#set headings for individual HA page
headings <- xpathSApply(IndividualHtml, "//strong", xmlValue)
#get the non strong values
values <- xpathSApply(IndividualHtml, "//strong/following-sibling::text()[1]", xmlValue)
#parse the title of the housing association from the page
title <- gsub("\n", "",  xpathSApply(IndividualHtml, "//h1[1]", xmlValue))
#write these to a data frame
d <- tryCatch(data.frame(values, row.names=headings),  error=function(e) NULL)
#write table to file with url as name
write.csv(d, file= paste(title, ".csv", sep=""), na="NA")
}
lapply(links, parsePage)
this <- data.frame(read.csv("B3Living.csv"))
this
ls
this <- data.frame(read.csv("First Choice Homes Oldham.csv"))
View(this)
links
library(XML)
#set WD
setwd("~/Documents/R/Housing-association-data")
#get all the urls for HAs from this page: "http://www.theguardian.com/housing-network/2012/jun/08/housing-association-profiles"
indexUrl <- "http://www.theguardian.com/housing-network/2012/jun/08/housing-association-profiles"
#set html to the url for individual HA record
html <- htmlTreeParse(indexUrl, useInternalNodes=T)
#use xpath to get all of the links for each HA record
links <- xpathSApply(html, "//p//a", xmlGetAttr, "href")
links
library(XML)
#set WD
setwd("~/Documents/R/Housing-association-data")
#get all the urls for HAs from this page: "http://www.theguardian.com/housing-network/2012/jun/08/housing-association-profiles"
indexUrl <- "http://www.theguardian.com/housing-network/2012/jun/08/housing-association-profiles"
#set html to the url for individual HA record
html <- htmlTreeParse(indexUrl, useInternalNodes=T)
#use xpath to get all of the links for each HA record
links <- xpathSApply(html, "//p//a", xmlGetAttr, "href")
#loop through each link, and return the data table for each HA
parsePage <- function(links) {
#set html to the url for individual HA record
IndividualHtml <- htmlTreeParse(links, useInternalNodes=T)
#set headings for individual HA page
headings <- xpathSApply(IndividualHtml, "//strong", xmlValue)
#get the non strong values
values <- xpathSApply(IndividualHtml, "//strong/following-sibling::text()[1]", xmlValue)
#parse the title of the housing association from the page
title <- gsub("\n", "",  xpathSApply(IndividualHtml, "//h1[1]", xmlValue))
#write these to a data frame
d <- tryCatch(data.frame(values, row.names=headings),  error=function(e) NULL)
#write table to file with url as name
write.csv(d, file= paste(title, ".csv", sep=""), na="NA")
}
parsePage(links[45:124])
lapply(links[45:124], parsePage)
temp = list.files(pattern="*.csv")
myFiles = lapply(temp, read.csv)
list.files()
files <- list.files()
files
data <- lapply(files, read.csv)
read.csv(files[3])
read.csv(files[3], )
files[3]
?read.csv
read.csv("Yorkshire Housing.csv")
read.csv(files[5])
read.csv(files[6])
read.csv(files[6])
read.csv(files[15])
lapply(files, read.csv)
files[1]
read.csv(files[1])
read.csv(files[2])
read.csv(files[3])
read.csv(files[4])
read.csv(files[5])
read.csv(files[4])
read.csv(files[3])
files[3]
lapply(files, file,info$size)
file.info(files[3])
file.info(files[2])
file.info(files[1])
file.info(files[4])
file.info(files[1])
file.info(files[7])
lapply(files, file.info)
lapply(files, file.info$size)
lapply(files, file.info()$size)
info = file.info(files)
info
row.names(info[info$siz == 3, ])
empty <- row.names(info[info$siz == 3, ])
file.remove(empty)
files
list.files()
files <- list.files()
dataSets <- lapply(files, read.csv)
dataSets
dataSets[turnover[]]
dataSets[Turnover]
dataSets["Turnover"]
library(plyr)
install.packages("plyr")
library(plyr)
arrange(dataSets, Turnover)
lapply(dataSets, data.frame)
arrange(dataSets, Turnover)
dataSets <- lapply(dataSets, data.frame)
dataSets
arrange(dataSets, Turnover)
is.data.frame(dataSets[1])
is.data.frame(dataSets[2])
is.data.frame(dataSets[3])
is.data.frame(dataSets[4])
dataSets
dataSets[1]
?data.frame
data.frame(dataSets[1])
thing <- data.frame(dataSets[1])
is.data.frame(thing)
dataSets
ldply(dataSets)
is.data.frame(dataSets)
data.frame(dataSets[2])
is.data.frame(dataSets[2])
is.data.frame(thing)
